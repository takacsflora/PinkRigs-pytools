{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet structure\n",
    "The main thing to know about the PinkRigs organisation is that we store two types of data about the experiments: \n",
    "- metadata (CSVs) summarising each animal and experiemntal session\n",
    "- experimental session data\n",
    "Typically when you run analysis on the PinkRigs data you will need to do the following: \n",
    "1) query the metadata to make sure you included all the data that fits your requrements\n",
    "2) load in the details (events,spikes,cameras) of the datasets that you have selected\n",
    "\n",
    "\n",
    "### Querying experiments\n",
    "You can query the experiements using the `query.queryCSV` module, e.g.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinkrigs_tools.dataset.query import queryCSV\n",
    "\n",
    "exp = queryCSV(\n",
    "  subject='AV043',\n",
    "  expDate='2024-03-14:2024-03-24', \n",
    "  expDef = 'multiSpaceWorld_checker_training',\n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "You can also direcrly query and then load the ONE folder content in one line using `load_data`. To specify the ONE folder content to load, you need to give a nested dictionary to the `data_name_dict` argument of the `load_data`. The nesting follows the ONE data structure `{collection:{'object':'attribute'}}`. For example: \n",
    "#### Events data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinkrigs_tools.dataset.query import load_data\n",
    "\n",
    "# define parameters of your query\n",
    "exp_kwargs = {\n",
    "    'subject': ['AV043'],\n",
    "    'expDate': '2024-03-14:2024-03-15',\n",
    "    }\n",
    "\n",
    "# define the ONE data to load\n",
    "data_name_dict = { 'events': {'_av_trials': 'table'}}\n",
    "recordings = load_data(data_name_dict=data_name_dict,**exp_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spikes data\n",
    "(this operation is the slowest! So, in order to avoid loading in unwanted data, you should probably query the data first and then only load in spike data for datasets that you ensured you want to use in your analysis.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_dict = {'spikes':'all','clusters':'all'}\n",
    "# both probes \n",
    "data_name_dict = {'probe0':ephys_dict,'probe1':ephys_dict} \n",
    "recordings = load_data(data_name_dict=data_name_dict,**exp_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Camera data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = ['frontCam','sideCam','eyeCam']\n",
    "data_name_dict = {cam:{'camera':['times','ROIMotionEnergy']} for cam in cameras}\n",
    "recordings = load_data(data_name_dict=data_name_dict,**exp_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can also first query the data using `queryCSV`, subset your DataFrame as you wish, and load the ONE object only on your subset using 'load_data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordings = load_data(recordings=exp.iloc[0:1], data_name_dict = {'events':{'_av_trials':'all'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or just load every data together! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# define which data you need\u001b[39;00m\n\u001b[0;32m      3\u001b[0m exp_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAV043\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpDate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-03-14\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpNum\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m     }\n\u001b[1;32m---> 10\u001b[0m recordings \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_name_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall-default\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexp_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Flora\\anaconda3\\envs\\neural_encoding\\lib\\site-packages\\pinkrigs_tools\\dataset\\query.py:309\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(recordings, data_name_dict, unwrap_probes, merge_probes, region_selection, filter_unique_shank_positions, cam_hierarchy, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m recordings \u001b[38;5;241m=\u001b[39m recordings\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;66;03m# set a copy to avoid pandas view versus copy warnings\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_name_dict:\n\u001b[1;32m--> 309\u001b[0m     collections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mdata_name_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m())\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collection \u001b[38;5;129;01min\u001b[39;00m collections:\n\u001b[0;32m    311\u001b[0m         recordings[collection] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "# define which data you need\n",
    "\n",
    "exp_kwargs = {\n",
    "    'subject': ['AV043'],\n",
    "    'expDate': '2024-03-14',\n",
    "    'expNum': '1'\n",
    "    }\n",
    "\n",
    "\n",
    "recordings = load_data(data_name_dict='all-default',**exp_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neurometric criteria of data selection\n",
    "(takes a long time!)\n",
    "Sometimes, you just want to call experimental sessions with neural data from a specific brain region, or ensure that each session you are calling is from a separate brain region. `load_data` can also handle that for you with some of its arguments that specifically relate to neural data. More broadly we deal with: \n",
    "- several probes per recordings: \n",
    "    - you can use the `unwrap_probes` argument to flatten the recordings DataFrame such that each probe is a separate row. In this case the neural data is merged under the `probe` column and the `probeID` column will contain info about which probe each row corresponds to (`probe0` or `probe1` on the ONE folder)\n",
    "    - you can also use the `merge_probes` to instead not create a sepatate row but just re-ID the clusters (adding 1000 to probe1 clusterIDs)\n",
    "- chronic recordings: \n",
    "    - `filter_unique_shank_positions` where we only allow each botrow position to be sampled once\n",
    "- region selection\n",
    "    to load experiments only when minimum 10 neurons etc. are in a particular brain region defined by Allen Acronyms. \n",
    "\n",
    "For Example the below code loads in all the data with minimum 20 neurons in MRN in `AV030`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_kwargs = {\n",
    "    'subject': ['AV030'],\n",
    "    'expDate': 'postImplant',\n",
    "    'expDef': 'multiSpaceWorld'\n",
    "    }\n",
    "\n",
    "data_name_dict = { 'events': {'_av_trials': 'table'}}\n",
    "\n",
    "# if you want ephys data\n",
    "ephys_dict = {'spikes':'all','clusters':'all'}\n",
    "# both probes \n",
    "ephys_dict = {'probe0':ephys_dict,'probe1':ephys_dict} \n",
    "data_name_dict.update(ephys_dict)\n",
    "\n",
    "# camera data\n",
    "cameras = ['frontCam','sideCam','eyeCam']\n",
    "cam_dict = {cam:{'camera':['times','ROIMotionEnergy']} for cam in cameras}\n",
    "data_name_dict.update(cam_dict)\n",
    "recordings = load_data(data_name_dict = data_name_dict,\n",
    "                             unwrap_probes= False,\n",
    "                             merge_probes=True,\n",
    "                             filter_unique_shank_positions = False,\n",
    "                             region_selection={'region_name':'MRN',\n",
    "                                                'framework':'Beryl',\n",
    "                                                'min_fraction':20,\n",
    "                                                'goodOnly':True,\n",
    "                                                'min_spike_num':300},\n",
    "                            **exp_kwargs\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call predtermined datasets \n",
    "Oftentimes I use neurometric criteria, but because it takes a long time, you want to compte the experiments that you want to analyse once, and then you can load just those experiments. For this, I also wrote a function ('dataset.pre_cured.call_') to call just predtermined fdatasets where I aleady set up the selection criteria. With this, you save your selection in your 'analysis_folder' and load summary data with the latest timestamp. You can recompute your selection using the 'recompute_data_selection' argument. For example, with the below code will call all the data where mice were recorded in the forebrain while doing the audiovisual task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataset.pre_cured import call_\n",
    "\n",
    "recordings = call_(subject_set='forebrain',\n",
    "                             dataset_type='active',\n",
    "                             spikeToInclde=True,\n",
    "                             camToInclude=False,\n",
    "                             recompute_data_selection=False,\n",
    "                             unwrap_probes= True,\n",
    "                             merge_probes=False,\n",
    "                             region_selection=None,\n",
    "                             filter_unique_shank_positions = True,\n",
    "                             analysis_folder = Path(r'D:\\ccCP\\forebrain'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_encoding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
